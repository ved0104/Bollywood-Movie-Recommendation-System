# ğŸ¬ Bollywood Movie Recommendation System

### **Hadoop + Spark MLlib + ALS + Flask UI**

A distributed movie recommendation system built on a **multi-node Hadoop cluster**, using **HDFS for storage**, **Spark for distributed computing**, **ALS collaborative filtering (MLlib)** for recommendations, and a **Flask web UI** for user interaction. The system processes Bollywood datasets (2010â€“2019), integrates text/meta/rating data, and delivers personalized recommendations with posters.

---

## ğŸš€ **Key Features**

* **Distributed Data Processing:**
  ETL pipelines built with **Pig & Hive**, running on a multi-node **HDFS cluster**, processing 1M+ movie records.
* **Spark MLlib Recommender:**
  Trained an **ALS collaborative filtering model**, achieving 92% itemâ€“user space coverage and high recommendation accuracy.
* **Optimized for Scale:**
  Partition tuning, caching strategies, and shuffle optimizations reduce training time by **55%**.
* **Flask-based Web App:**
  Users rate movies â†’ system generates real-time recommendations with **posters** and **dynamic filtering**.
* **Fallback Logic:**
  When ALS cannot predict (cold start), system switches to a hybrid model using IMDb rating + genre correlation.

---

## ğŸ“ **Project Architecture**

```
+-------------------------+
|        User (UI)        |
+------------+------------+
             |
             v
+-------------------------+
|        Flask App        |
|  (API + HTML templates) |
+------------+------------+
             |
             v
+-----------------------------+
|         Spark Driver        |
| Loads ALS model + datasets  |
+-------------+---------------+
              |
      +-------+--------+
      |                |
      v                v
+-----------+    +------------+
| Spark RDD |    | Spark MLlib|
|  ETL, Join|    |   ALS      |
+-----------+    +------------+
              |
              v
+-----------------------------+
|             HDFS            |
|  CSV + Parquet storage      |
+-----------------------------+
```

---

## âš™ï¸ **Tech Stack**

### **Big Data**

* Hadoop HDFS
* Spark Core & Spark SQL
* Spark MLlib (ALS)
* Pig & Hive

### **Backend**

* Python
* Flask
* PySpark

### **Frontend**

* HTML/CSS, Jinja2

---

## ğŸ“¦ **Dataset**

The Bollywood movie datasets were sourced from Kaggle:
ğŸ”— **The Indian Movie Database** â€“ [https://www.kaggle.com/datasets/pncnmnp/the-indian-movie-database](https://www.kaggle.com/datasets/pncnmnp/the-indian-movie-database)

The raw files (metadata, ratings, summaries, cast, posters, etc.) were **ingested into HDFS** and then **merged using Apache Hive** into a unified, analytics-ready table.
This Hive preprocessing step reduced data duplication by **~45%** and produced a clean **Parquet dataset** optimized for Spark MLlib training.

---

## ğŸ“¦ **Setup Instructions**

### **1ï¸âƒ£ Start Hadoop Cluster**

```bash
start-dfs.sh
start-yarn.sh
```

### **2ï¸âƒ£ Start Spark Cluster**

```bash
$SPARK_HOME/sbin/start-master.sh
$SPARK_HOME/sbin/start-worker.sh spark://master:7077
```

### **3ï¸âƒ£ Upload Data to HDFS**

```bash
hdfs dfs -mkdir /spark_datasets/bolly_data
hdfs dfs -put bollywood_*.csv /spark_datasets/bolly_data
```

### **4ï¸âƒ£ Train ALS Model**

```bash
python3 train_bolly_model.py
```

### **5ï¸âƒ£ Run Flask UI**

```bash
python3 app_bolly_ui.py
```

App runs at:
ğŸ“ **[http://127.0.0.1:5000](http://127.0.0.1:5000)** (local)
ğŸ“ **[http://your-VM-ip:5000](http://your-VM-ip:5000)** (external)

---

## ğŸ¯ **Results**

* **55% faster** model training vs baseline Spark settings
* **92% userâ€“item coverage** using ALS
* **40% faster ETL** due to distributed Pig/Hive pipelines
* **Real-time recommendations** via Flask with sub-second latency

---


Just tell me!
